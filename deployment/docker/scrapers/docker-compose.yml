# Docker Compose for Project Synapse Scrapers
# Provides complete development and production deployment setup

version: '3.8'

services:
  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: synapse-postgres
    environment:
      POSTGRES_DB: synapse
      POSTGRES_USER: synapse
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-synapse_dev_password}
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8 --lc-collate=C --lc-ctype=C"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/init-db.sql:ro
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U synapse -d synapse"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - synapse-network
    restart: unless-stopped

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: synapse-redis
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD:-redis_dev_password}
    volumes:
      - redis_data:/data
    ports:
      - "${REDIS_PORT:-6379}:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - synapse-network
    restart: unless-stopped

  # Scraper Service (Production)
  scrapers:
    build:
      context: ../../../
      dockerfile: deployment/docker/scrapers/Dockerfile
      target: production
      args:
        BUILD_DATE: ${BUILD_DATE:-$(date -u +'%Y-%m-%dT%H:%M:%SZ')}
        VCS_REF: ${VCS_REF:-$(git rev-parse --short HEAD)}
        VERSION: ${VERSION:-latest}
    container_name: synapse-scrapers
    environment:
      # Database Configuration
      DATABASE_URL: postgresql://synapse:${POSTGRES_PASSWORD:-synapse_dev_password}@postgres:5432/synapse
      
      # Redis Configuration
      REDIS_URL: redis://:${REDIS_PASSWORD:-redis_dev_password}@redis:6379/0
      
      # Scraper Configuration
      SCRAPER_WORKERS: ${SCRAPER_WORKERS:-4}
      SCRAPER_TIMEOUT: ${SCRAPER_TIMEOUT:-30}
      SCRAPER_MAX_RETRIES: ${SCRAPER_MAX_RETRIES:-3}
      
      # Logging Configuration
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      
      # Health Check Configuration
      HEALTH_CHECK_PORT: 8080
      
      # Feature Flags
      RUN_MIGRATIONS: ${RUN_MIGRATIONS:-false}
      DEBUG: ${DEBUG:-false}
      
      # Build Information
      BUILD_DATE: ${BUILD_DATE:-$(date -u +'%Y-%m-%dT%H:%M:%SZ')}
      VCS_REF: ${VCS_REF:-$(git rev-parse --short HEAD)}
      VERSION: ${VERSION:-latest}
    volumes:
      - scraper_logs:/app/logs
      - scraper_data:/app/data
      - scraper_tmp:/app/tmp
    ports:
      - "${SCRAPER_HEALTH_PORT:-8080}:8080"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "healthcheck.py"]
      interval: 30s
      timeout: 10s
      start_period: 60s
      retries: 3
    networks:
      - synapse-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 256M

  # Scraper Service (Development)
  scrapers-dev:
    build:
      context: ../../../
      dockerfile: deployment/docker/scrapers/Dockerfile
      target: development
      args:
        BUILD_DATE: ${BUILD_DATE:-$(date -u +'%Y-%m-%dT%H:%M:%SZ')}
        VCS_REF: ${VCS_REF:-$(git rev-parse --short HEAD)}
        VERSION: ${VERSION:-dev}
    container_name: synapse-scrapers-dev
    environment:
      # Database Configuration
      DATABASE_URL: postgresql://synapse:${POSTGRES_PASSWORD:-synapse_dev_password}@postgres:5432/synapse
      
      # Redis Configuration
      REDIS_URL: redis://:${REDIS_PASSWORD:-redis_dev_password}@redis:6379/0
      
      # Development Configuration
      DEBUG: "true"
      LOG_LEVEL: DEBUG
      RUN_MIGRATIONS: "true"
      
      # Scraper Configuration
      SCRAPER_WORKERS: 2
      SCRAPER_TIMEOUT: 10
      SCRAPER_MAX_RETRIES: 2
      
      # Health Check Configuration
      HEALTH_CHECK_PORT: 8080
    volumes:
      - ../../../src:/app/src:ro
      - ../../../tests:/app/tests:ro
      - ../../../alembic:/app/alembic:ro
      - ../../../alembic.ini:/app/alembic.ini:ro
      - scraper_logs:/app/logs
      - scraper_data:/app/data
      - scraper_tmp:/app/tmp
    ports:
      - "${SCRAPER_DEV_HEALTH_PORT:-8081}:8080"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - synapse-network
    profiles:
      - development
    command: ["shell"]

  # Monitoring (Optional)
  prometheus:
    image: prom/prometheus:latest
    container_name: synapse-prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    networks:
      - synapse-network
    profiles:
      - monitoring
    restart: unless-stopped

  # Grafana (Optional)
  grafana:
    image: grafana/grafana:latest
    container_name: synapse-grafana
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_USERS_ALLOW_SIGN_UP: "false"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    depends_on:
      - prometheus
    networks:
      - synapse-network
    profiles:
      - monitoring
    restart: unless-stopped

# Named volumes for data persistence
volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  scraper_logs:
    driver: local
  scraper_data:
    driver: local
  scraper_tmp:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

# Network configuration
networks:
  synapse-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16